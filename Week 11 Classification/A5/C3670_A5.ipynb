{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**COMP3670/6670 Assignment 5**\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Enter Your Student ID:**\n",
    "\n",
    "**Your Name:** \n",
    "    \n",
    "**Deadline:** N.A\n",
    "\n",
    "**Submit:** N.A\n",
    "    \n",
    "**Enter Discussion Partner IDs Below:**\n",
    "- <Enter ID 1>\n",
    "- <Enter ID 2>\n",
    "- <Enter ID 3>\n",
    "\n",
    "**Programming Section** (100%)\n",
    "- 1.1 = 5%\n",
    "- 1.2 = 5%\n",
    "- 1.3 = 10%\n",
    "- 1.4 = 15%\n",
    "- 1.5 = 5%\n",
    "- 1.6 = 10%\n",
    "- 2.1 = 50%\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "THEORY SECTION (0%)\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no theory questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "\n",
    "**PROGRAMMING SECTION (100%)**\n",
    "---\n",
    "\n",
    "For all of the following, program the solution yourself. Don't just call a library function that does the whole question for you, or you'll get zero (no, that doesn't mean you can't use any library functions, but it does mean that you have to show you understand how to compute the answer yourself).\n",
    "\n",
    "**All written answers** should be between 50 and 500 words. If you can describe all the necessary information in 50 words, that's better. However, you'll only be graded on whether you describe the necessary ideas.\n",
    "\n",
    "\n",
    "-----------\n",
    "\n",
    "   **TASK 0.1:** You know the drill. Import Numpy and PyPlot. We're also going to generate a dataset.\n",
    "\n",
    "\n",
    "-----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 1. ... 1. 0. 0.]\n",
      " [0. 0. 0. ... 1. 0. 0.]\n",
      " [1. 1. 1. ... 0. 0. 1.]\n",
      " ...\n",
      " [1. 1. 1. ... 1. 1. 0.]\n",
      " [0. 0. 1. ... 1. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "[[0. 0. 1. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 1. 0. 0.]\n",
      " ...\n",
      " [1. 1. 1. 1.]\n",
      " [0. 1. 0. 1.]\n",
      " [0. 0. 0. 0.]]\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D #This is for 3d scatter plots.\n",
    "import math\n",
    "import random\n",
    "from scipy.stats import multivariate_normal\n",
    "import os\n",
    "from matplotlib.pyplot import imread\n",
    "np.random.seed(13579201)\n",
    "\n",
    "k = 4\n",
    "n = 8\n",
    "m = 5000\n",
    "D = np.zeros((m, n))\n",
    "Y = np.zeros((m, k))\n",
    "for ix in range(0, m):\n",
    "    dpool = random.randint(0, 2**n)\n",
    "    ypool = round(np.sqrt(dpool))\n",
    "    for iy in range(n):\n",
    "        if dpool >= 2**(D.shape[1] - iy - 1):\n",
    "            D[ix, iy] = 1\n",
    "            dpool -= 2**(D.shape[1] - iy - 1)\n",
    "    for iy in range(Y.shape[1]):\n",
    "        if ypool >= 2**(Y.shape[1] - iy - 1):\n",
    "            reading = ((np.random.randn() + 0.3) > 0)\n",
    "            Y[ix, iy - k] = 1 * reading\n",
    "            ypool -= 2**(Y.shape[1] - iy - 1)\n",
    "\n",
    "print(D)\n",
    "print(Y)\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PROGRAMMING EXERCISE 1  \n",
    "-----------\n",
    "\n",
    "Note that for all of the questions that follow, you may re-use any code from your previous assignments.\n",
    "\n",
    "We're going to implement logistic regression with gradient descent in this first exercise.\n",
    "\n",
    "It's much like linear regression, but we apply a sigmoid function to the output, which of course changes the gradient!\n",
    "\n",
    "The point of logistic regression is to predict a label {0, 1} or a probability.\n",
    "\n",
    "We're going to re-use the dataset from assignment 5 for this problem:\n",
    "\n",
    "$D \\in \\mathbb{R}^{m \\times n}$ together with $Y \\in \\mathbb{R}^{m \\times k}$ is our dataset.\n",
    "\n",
    "Our objective is to predict $Y$ given $D$, such that $s(D\\theta) = Y$, where $s()$ is the same logistic or sigmoid function you used in assignment 4.\n",
    "\n",
    "We're going to use logistic regression to predict a binary string!\n",
    "\n",
    "\n",
    "---\n",
    "   **TASK 1.1:** Complete the function $initialise(D, Y) = \\theta$, which detects the dimensions of $D$ and $Y$ and generates $\\theta$ of appropriate dimension, with random values appropriate for regression, such that the dimensions of $D\\theta$ is the same as the dimensions of $Y$.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialise(D, Y):\n",
    "    theta = None\n",
    "    #YOUR CODE HERE\n",
    "    return theta\n",
    "\n",
    "theta = initialise(D, Y)\n",
    "print(theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "**TASK 1.2:** Complete $s(Z) = sZ$, which accepts a matrix $Z$ and outputs $sZ$, where $sZ$ is just $Z$ with the sigmoid (or \"logistic\") function applied to every element.\n",
    "\n",
    "You must do this without using a loop.\n",
    "\n",
    "**HINT:** \n",
    "- Recall assignment 4, where you applied the sigmoid function in exactly the same manner.\n",
    "- You might want to use np.vectorize(). You'll probably need to define a second function which accepts a single element and applies the sigmoid function to that element (this is how np.vectorize works).\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def s(Z):\n",
    "    sZ = None\n",
    "    #YOUR CODE HERE\n",
    "    return sZ\n",
    "\n",
    "print((s(D@theta * 999999999)).astype('int'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall assignment 4, where you implemented a neural network where the $X\\theta^{[1]}$ had a sigmoid function applied to its output.\n",
    "\n",
    "Thus the first layer was the function $s(X\\theta^{[1]})$.\n",
    "\n",
    "That first layer was performing logistic regression with mean squared error!\n",
    "\n",
    "Now, for what follows you may choose any convex loss function you wish. For instance, you might:\n",
    "\n",
    "1. Implement the mean squared loss function as you did in assignment 4 exercise 2 (this will only get you half marks).\n",
    "2. Implement the cross-entropy loss (this is the most common loss function for logistic regression, and I recommend you implement this).\n",
    "3. Implement anything else your heart desires, so long as it functions as a convex loss function for logistic regression!\n",
    "\n",
    "Not sure what convex is? Then you must look this up. Convex means the loss function has only one global minimum!\n",
    "\n",
    "Likewise with cross entropy loss; you need to research and understand this on your own. \n",
    "\n",
    "Cross entropy loss when $k = 1$ is computed as follows:\n",
    "\n",
    "$\\mathcal{L}(\\theta) = -\\frac{1}{m}\\sum_{i=0}^m \\left[y_i ln (s(d_i\\theta)) + (1 - y_i) ln (1 - s(d_i\\theta))\\right] + \\frac{\\gamma}{2}\\theta^T\\theta$\n",
    "\n",
    "... where $ln(z)$ is the natural logarithm of a scalar $z$.\n",
    "\n",
    "When $k =1$, $s(d_i\\theta)$ and $y_i$ are scalars. As $k > 1$, you'll need to modify the above loss function to work with vectors.\n",
    "\n",
    "---\n",
    "\n",
    "**TASK 1.3:** Complete the function $\\mathcal{L}(\\theta) = loss(D, \\theta, Y) = l$, where $l$ is a scalar that represents the loss. That means $l$ is the sum of the $k$ losses (because we're simultaneously running $k$ regressions again).\n",
    "\n",
    "You will get 50% for implementing the mean squared loss function you used in assignment 4 exercise 2, or 100% for implementing a different loss function (so long as it works).\n",
    "\n",
    "Bear in mind you will need to compute $\\frac{d \\mathcal{L}(\\theta)}{d \\theta}$ for this loss function in order to complete the next question, so pick something you actually know how to compute the derivative of.\n",
    "\n",
    "You must also include a form of regularisation such as $\\gamma \\theta^T \\theta$, where $\\gamma$ is a scalar chosen by you (I suggest something like 0.0001). When computing regularisation, remember that $k > 1$. 20% of your grade will be deducted for failing to implement a form of regularisation.\n",
    "\n",
    "**HINT:** If implementing cross entropy loss, you'll need to account for taking the log of $0$.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(D, theta, Y):\n",
    "    l = None\n",
    "    #YOUR CODE HERE\n",
    "    return l\n",
    "\n",
    "l = loss(D, theta, Y)\n",
    "print(l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**TASK 1.4:** Implement $gradient\\_update(D, \\theta, Y, \\lambda) = \\theta - \\lambda \\frac{d \\mathcal{L}(\\theta)}{d \\theta} = \\theta_{new}$, which updates $\\theta$ using the gradient of the loss function you implemented above.\n",
    "\n",
    "It must be the gradient of the loss function you implemented, not a different loss function.\n",
    "\n",
    "$\\lambda$ is the learning rate.\n",
    "\n",
    "As above, if it's just the mean squared loss you will only be eligible for 50% of the grade, and 20% of your grade will be deducted for not including a form of regularisation.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_update(D, theta, Y, lam):\n",
    "    theta_new = None\n",
    "    #YOUR CODE HERE\n",
    "    return theta_new\n",
    "\n",
    "theta_new_test = gradient_update(D, theta, Y, 0.05)\n",
    "print(theta_new_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**TASK 1.5:** Complete the function $predict(d, \\theta) = s(d\\theta) = y$, where $y \\in \\{0, 1\\}^{(p \\times k)}$ and $d \\in \\{0, 1\\}^{(p \\times n)}$ and $1 \\ge p \\ge m$.\n",
    "\n",
    "Note, every element of the output must be an integer.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(d, theta):\n",
    "    y = None\n",
    "    #YOUR CODE HERE\n",
    "    return y\n",
    "\n",
    "print(predict(D[0:7, :], theta))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**TASK 1.6:** Complete the function $gradient\\_descent(D, Y, lam, iterations) = \\theta$, which  uses the functions above to find $\\theta$ such that $predict(D, \\theta) = Y$.\n",
    "\n",
    "As in assignment 4, record the loss at each iteration and store the values in the array $losses$.\n",
    "\n",
    "You need to make sure your loss decreases in monotone downward trajectory. If it increases at any point, something has gone wrong.\n",
    "\n",
    "You will be assessed on whether, overall, you've succeeded in implementing logistic regression (not just the gradient descent algorithm, which you've done before in any case).\n",
    "\n",
    "**HINT:** \n",
    "- Remember, at the start of this assignment we said you could re-use your code from previous assignments.\n",
    "- Also, it's impossible to get zero error. You need to just minimise the error.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def gradient_descent(D, Y, lam, iterations):\n",
    "    theta = initialise(D, Y)\n",
    "    losses = np.zeros((iterations))\n",
    "    #YOUR CODE HERE\n",
    "    return theta, losses\n",
    "\n",
    "lam = 0.3 #Learning rate\n",
    "iterations = 60 #Number of iterations of gradient descent\n",
    "\n",
    "#For the purposes of doing this assignment, this code isn't really here. Pretend it's engraved in rock.\n",
    "theta, losses = gradient_descent(D, Y, lam, iterations)\n",
    "plt.plot(losses)\n",
    "plt.show()\n",
    "print(theta)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
